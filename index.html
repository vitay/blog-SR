<!DOCTYPE html>
<!--
==============================================================================
           "GitHub HTML5 Pandoc Template" v2.2 — by Tristano Ajmone
==============================================================================
Copyright © Tristano Ajmone, 2017-2020, MIT License (MIT). Project's home:

- https://github.com/tajmone/pandoc-goodies

The CSS in this template reuses source code taken from the following projects:

- GitHub Markdown CSS: Copyright © Sindre Sorhus, MIT License (MIT):
  https://github.com/sindresorhus/github-markdown-css

- Primer CSS: Copyright © 2016-2017 GitHub Inc., MIT License (MIT):
  http://primercss.io/

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
The MIT License

Copyright (c) Tristano Ajmone, 2017-2020 (github.com/tajmone/pandoc-goodies)
Copyright (c) Sindre Sorhus <sindresorhus@gmail.com> (sindresorhus.com)
Copyright (c) 2017 GitHub Inc.

"GitHub Pandoc HTML5 Template" is Copyright (c) Tristano Ajmone, 2017-2020,
released under the MIT License (MIT); it contains readaptations of substantial
portions of the following third party softwares:

(1) "GitHub Markdown CSS", Copyright (c) Sindre Sorhus, MIT License (MIT).
(2) "Primer CSS", Copyright (c) 2016 GitHub Inc., MIT License (MIT).

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.
==============================================================================-->
<html>
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Julien Vitay - TU Chemnitz" />
  <title>Successor Representations</title>
  <style type="text/css">
@charset "UTF-8";.markdown-body{-ms-text-size-adjust:100%;-webkit-text-size-adjust:100%;color:#24292e;font-family:-apple-system,system-ui,BlinkMacSystemFont,"Segoe UI",Helvetica,Arial,sans-serif,"Apple Color Emoji","Segoe UI Emoji","Segoe UI Symbol";font-size:16px;line-height:1.5;word-wrap:break-word;box-sizing:border-box;min-width:200px;max-width:980px;margin:0 auto;padding:45px}.markdown-body a{color:#0366d6;background-color:transparent;text-decoration:none;-webkit-text-decoration-skip:objects}.markdown-body a:active,.markdown-body a:hover{outline-width:0}.markdown-body a:hover{text-decoration:underline}.markdown-body a:not([href]){color:inherit;text-decoration:none}.markdown-body strong{font-weight:600}.markdown-body h1,.markdown-body h2,.markdown-body h3,.markdown-body h4,.markdown-body h5,.markdown-body h6{margin-top:24px;margin-bottom:16px;font-weight:600;line-height:1.25}.markdown-body h1{font-size:2em;margin:.67em 0;padding-bottom:.3em;border-bottom:0px solid #eaecef}.markdown-body h2{padding-bottom:.3em;font-size:1.5em;border-bottom:0px solid #eaecef}.markdown-body h3{font-size:1.25em}.markdown-body h4{font-size:1em}.markdown-body h5{font-size:.875em}.markdown-body h6{font-size:.85em;color:#6a737d}.markdown-body img{border-style:none}.markdown-body svg:not(:root){overflow:hidden}.markdown-body hr{box-sizing:content-box;height:.25em;margin:24px 0;padding:0;overflow:hidden;background-color:#e1e4e8;border:0}.markdown-body hr::before{display:table;content:""}.markdown-body hr::after{display:table;clear:both;content:""}.markdown-body input{margin:0;overflow:visible;font:inherit;font-family:inherit;font-size:inherit;line-height:inherit}.markdown-body [type=checkbox]{box-sizing:border-box;padding:0}.markdown-body *{box-sizing:border-box}.markdown-body blockquote{margin:0}.markdown-body ol,.markdown-body ul{padding-left:2em}.markdown-body ol ol,.markdown-body ul ol{list-style-type:lower-roman}.markdown-body ol ol,.markdown-body ol ul,.markdown-body ul ol,.markdown-body ul ul{margin-top:0;margin-bottom:0}.markdown-body ol ol ol,.markdown-body ol ul ol,.markdown-body ul ol ol,.markdown-body ul ul ol{list-style-type:lower-alpha}.markdown-body li>p{margin-top:16px}.markdown-body li+li{margin-top:.25em}.markdown-body dd{margin-left:0}.markdown-body dl{padding:0}.markdown-body dl dt{padding:0;margin-top:16px;font-size:1em;font-style:italic;font-weight:600}.markdown-body dl dd{padding:0 16px;margin-bottom:16px}.markdown-body code{font-family:SFMono-Regular,Consolas,"Liberation Mono",Menlo,Courier,monospace}.markdown-body pre{font:12px SFMono-Regular,Consolas,"Liberation Mono",Menlo,Courier,monospace;word-wrap:normal}.markdown-body blockquote,.markdown-body dl,.markdown-body ol,.markdown-body p,.markdown-body pre,.markdown-body table,.markdown-body ul{margin-top:0;margin-bottom:16px}.markdown-body blockquote{padding:0 1em;color:#6a737d;border-left:.25em solid #dfe2e5}.markdown-body blockquote>:first-child{margin-top:0}.markdown-body blockquote>:last-child{margin-bottom:0}.markdown-body table{display:block;width:100%;overflow:auto;border-spacing:0;border-collapse:collapse}.markdown-body table th{font-weight:600}.markdown-body table td,.markdown-body table th{padding:6px 13px;border:1px solid #dfe2e5}.markdown-body table tr{background-color:#fff;border-top:1px solid #c6cbd1}.markdown-body table tr:nth-child(2n){background-color:#f6f8fa}.markdown-body img{max-width:100%;box-sizing:content-box;background-color:#fff}.markdown-body code{padding:.2em 0;margin:0;font-size:85%;background-color:rgba(27,31,35,.05);border-radius:3px}.markdown-body code::after,.markdown-body code::before{letter-spacing:-.2em;content:" "}.markdown-body pre>code{padding:0;margin:0;font-size:100%;word-break:normal;white-space:pre;background:0 0;border:0}.markdown-body .highlight{margin-bottom:16px}.markdown-body .highlight pre{margin-bottom:0;word-break:normal}.markdown-body .highlight pre,.markdown-body pre{padding:16px;overflow:auto;font-size:85%;line-height:1.45;background-color:#f6f8fa;border-radius:3px}.markdown-body pre code{display:inline;max-width:auto;padding:0;margin:0;overflow:visible;line-height:inherit;word-wrap:normal;background-color:transparent;border:0}.markdown-body pre code::after,.markdown-body pre code::before{content:normal}.markdown-body .full-commit .btn-outline:not(:disabled):hover{color:#005cc5;border-color:#005cc5}.markdown-body kbd{box-shadow:inset 0 -1px 0 #959da5;display:inline-block;padding:3px 5px;font:11px/10px SFMono-Regular,Consolas,"Liberation Mono",Menlo,Courier,monospace;color:#444d56;vertical-align:middle;background-color:#fcfcfc;border:1px solid #c6cbd1;border-bottom-color:#959da5;border-radius:3px;box-shadow:inset 0 -1px 0 #959da5}.markdown-body :checked+.radio-label{position:relative;z-index:1;border-color:#0366d6}.markdown-body .task-list-item{list-style-type:none}.markdown-body .task-list-item+.task-list-item{margin-top:3px}.markdown-body .task-list-item input{margin:0 .2em .25em -1.6em;vertical-align:middle}.markdown-body::before{display:table;content:""}.markdown-body::after{display:table;clear:both;content:""}.markdown-body>:first-child{margin-top:0!important}.markdown-body>:last-child{margin-bottom:0!important}.Alert,.Error,.Note,.Success,.Warning{padding:11px;margin-bottom:24px;border-style:solid;border-width:1px;border-radius:4px}.Alert p,.Error p,.Note p,.Success p,.Warning p{margin-top:0}.Alert p:last-child,.Error p:last-child,.Note p:last-child,.Success p:last-child,.Warning p:last-child{margin-bottom:0}.Alert{color:#246;background-color:#e2eef9;border-color:#bac6d3}.Warning{color:#4c4a42;background-color:#fff9ea;border-color:#dfd8c2}.Error{color:#911;background-color:#fcdede;border-color:#d2b2b2}.Success{color:#22662c;background-color:#e2f9e5;border-color:#bad3be}.Note{color:#2f363d;background-color:#f6f8fa;border-color:#d5d8da}.Alert h1,.Alert h2,.Alert h3,.Alert h4,.Alert h5,.Alert h6{color:#246;margin-bottom:0}.Warning h1,.Warning h2,.Warning h3,.Warning h4,.Warning h5,.Warning h6{color:#4c4a42;margin-bottom:0}.Error h1,.Error h2,.Error h3,.Error h4,.Error h5,.Error h6{color:#911;margin-bottom:0}.Success h1,.Success h2,.Success h3,.Success h4,.Success h5,.Success h6{color:#22662c;margin-bottom:0}.Note h1,.Note h2,.Note h3,.Note h4,.Note h5,.Note h6{color:#2f363d;margin-bottom:0}.Alert h1:first-child,.Alert h2:first-child,.Alert h3:first-child,.Alert h4:first-child,.Alert h5:first-child,.Alert h6:first-child,.Error h1:first-child,.Error h2:first-child,.Error h3:first-child,.Error h4:first-child,.Error h5:first-child,.Error h6:first-child,.Note h1:first-child,.Note h2:first-child,.Note h3:first-child,.Note h4:first-child,.Note h5:first-child,.Note h6:first-child,.Success h1:first-child,.Success h2:first-child,.Success h3:first-child,.Success h4:first-child,.Success h5:first-child,.Success h6:first-child,.Warning h1:first-child,.Warning h2:first-child,.Warning h3:first-child,.Warning h4:first-child,.Warning h5:first-child,.Warning h6:first-child{margin-top:0}h1.title,p.subtitle{text-align:center}h1.title.followed-by-subtitle{margin-bottom:0}p.subtitle{font-size:1.5em;font-weight:600;line-height:1.25;margin-top:0;margin-bottom:16px;padding-bottom:.3em}div.line-block{white-space:pre-line}
  </style>
  <style type="text/css">code{white-space: pre;}</style>
  <link rel="stylesheet" href="assets/github.css">
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
  
  
  
  
  
  
  
  
  
  
  
</head>
<body>
<article class="markdown-body">
<header>
<h1 class="title followed-by-subtitle">Successor Representations</h1>
<p class="subtitle">An introduction</p>
<p class="author">Julien Vitay - TU Chemnitz</p>
<div class="summary">
Successor representations (SR) attract a lot of attention these days, both in the neuroscientific and machine learning / deep RL communities. This post is intended to explain the main difference between SR and model-free / model-based RL algorithms and to point out its usefulness to understand goal-directed behavior.
</div>
</header>
<hr>
<nav id="TOC">
<!--<h2 class="toc-title">Contents</h2>-->
<ul>
<li><a href="#motivation"><span class="toc-section-number">1</span> Motivation</a>
<ul>
<li><a href="#model-free-vs.-model-based-rl"><span class="toc-section-number">1.1</span> Model-free vs. model-based RL</a></li>
<li><a href="#goal-directed-behavior-vs.-habits"><span class="toc-section-number">1.2</span> Goal-directed behavior vs. habits</a></li>
</ul></li>
<li><a href="#successor-representations-in-reinforcement-learning"><span class="toc-section-number">2</span> Successor representations in reinforcement learning</a>
<ul>
<li><a href="#main-idea"><span class="toc-section-number">2.1</span> Main idea</a></li>
<li><a href="#linear-function-approximation"><span class="toc-section-number">2.2</span> Linear function approximation</a></li>
<li><a href="#successor-representations-of-actions"><span class="toc-section-number">2.3</span> Successor representations of actions</a></li>
</ul></li>
<li><a href="#successor-representations-in-neuroscience"><span class="toc-section-number">3</span> Successor representations in neuroscience</a>
<ul>
<li><a href="#human-goal-directed-behavior"><span class="toc-section-number">3.1</span> Human goal-directed behavior</a></li>
<li><a href="#neural-substrates-of-successor-representations"><span class="toc-section-number">3.2</span> Neural substrates of successor representations</a>
<ul>
<li><a href="#dopamine-as-a-spe"><span class="toc-section-number">3.2.1</span> Dopamine as a SPE</a></li>
<li><a href="#hippocampus-as-a-predictive-map"><span class="toc-section-number">3.2.2</span> Hippocampus as a predictive map</a></li>
</ul></li>
</ul></li>
<li><a href="#discussion"><span class="toc-section-number">4</span> Discussion</a></li>
<li><a href="#references">References</a></li>
</ul>
</nav>
<hr>
<h1 data-number="1" id="motivation"><span class="header-section-number">1</span> Motivation</h1>
<h2 data-number="1.1" id="model-free-vs.-model-based-rl"><span class="header-section-number">1.1</span> Model-free vs. model-based RL</h2>
<p>There are two main families of <strong>reinforcement learning</strong> (RL) <span class="citation" data-cites="Sutton2017">(<a href="#ref-Sutton2017" role="doc-biblioref">Sutton and Barto, 2017</a>)</span> algorithms:</p>
<ul>
<li><strong>Model-free</strong> (MF) methods estimate the value of a state <span class="math inline">\(V^\pi(s)\)</span> or of a state-action pair <span class="math inline">\(Q^\pi(s, a)\)</span> by sampling trajectories and averaging the obtained returns (Monte-Carlo control), or by estimating the Bellman equations (Temporal difference - TD):</li>
</ul>
<p><span class="math display">\[V^\pi(s) = \mathbb{E}_{\pi} [\sum_{k=0} \gamma^k \, r_{t+k+1}  | s_t = s] = \mathbb{E}_{\pi} [r_{t+1} + \gamma \, V^\pi(s_{t+1})  | s_t = s]\]</span></p>
<p><span class="math display">\[Q^\pi(s, a) = \mathbb{E}_{\pi} [\sum_{k=0} \gamma^k \, r_{t+k+1}  | s_t = s, a_t = a] = \mathbb{E}_{\pi} [r_{t+1} + \gamma \, Q^\pi(s_{t+1}, a_{t+1})  | s_t = s, a_t = a]\]</span></p>
<ul>
<li><strong>Model-based</strong> (MB) methods use (or learn) a model of the environment - transition probabilities <span class="math inline">\(p(s\_{t+1} | s\_t, a\_t)\)</span> and reward probabilities <span class="math inline">\(r(s\_t, a\_t, s\_{t+1})\)</span> - and use it to plan trajectories maximizing the theoretical return, either through some form of forward planning (search tree) or using dynamic programming (solving the Bellman equations directly).</li>
</ul>
<p><span class="math display">\[\pi^* = \text{argmax}_{\pi} \; p(s_0) \, \sum_{t=0}^\infty \gamma^t \, p(s_{t+1} | s_t, a_t) \, \pi(s_t, a_t) \, r(s_t, a_t, s_{t+1})\]</span></p>
<p><span class="math display">\[V^*(s) = \max_a \sum_{s&#39;} p(s&#39; | s, a) \, (r(s_t, a, s&#39;)+ \gamma \, V^*(s&#39;))\]</span></p>
<p>The main advantage of model-free methods is their speed: they <em>cache</em> the future of the system into value functions. When having to take a decision at time <span class="math inline">\(t\)</span>, we only need to look at the action with the highest Q-value in the state <span class="math inline">\(s\_t\)</span> and take it. If the Q-values are optimal, this is the optimal policy. Oppositely, model-based algorithms have to plan sequentially in the state-action space, which can be very long if the problem has a long temporal horizon.</p>
<p>The main drawback of MF methods is their <em>inflexibility</em> when the reward distribution changes. When the reward associated with a transition changes (the source of reward has vanished, its nature has changed, the rules of the game have changed, etc), each action leading to that transition has to be experienced multiple times before the corresponding values reflect that change. This is due to the use of the <strong>temporal difference</strong> (TD) algorithm, where the <strong>reward prediction error</strong> (RPE) is used to update values:</p>
<p><span class="math display">\[\delta_t = r_{t+1} + \gamma \, V^\pi(s_{t+1}) - V^\pi(s_t)\]</span></p>
<p><span class="math display">\[\Delta V^\pi(s_t) = \alpha \, \delta_t\]</span></p>
<p>When the reward associated to a transition changes drastically, only the last state (or action) is updated after that experience (unless we use eligibility traces). Only multiple repetitions of the same trajectory would allow changing the initial decisions. This is opposite to MB methods, where a change in the reward distribution would very quickly influence the planning of the optimal trajectory. In MB, the reward probabilities can be estimated with:</p>
<p><span class="math display">\[
    \Delta r(s_t, a_t, s_{t+1}) = \alpha \, (r_{t+1} - r(s_t, a_t, s_{t+1}))
\]</span></p>
<p>with <span class="math inline">\(r_{t+1}\)</span> being the reward obtained during one sampled transition. The transition probabilities can also be learned from experience using:</p>
<p><span class="math display">\[
    \Delta p(s&#39; | s_t, a_t) = \alpha \, (\mathbb{I}(s_{t+1} = s&#39;) - p(s&#39; | s_t, a_t))
\]</span></p>
<p>where <span class="math inline">\(\mathbb{I}(b)\)</span> is 1 when <span class="math inline">\(b\)</span> is true, 0 otherwise. Depending on the learning rate, changes in the environment dynamics can be very quickly learned by MB methods, as updates do not depend on other estimates (there is no bootstrapping contrary to TD).</p>
<h2 data-number="1.2" id="goal-directed-behavior-vs.-habits"><span class="header-section-number">1.2</span> Goal-directed behavior vs. habits</h2>
<p>The model-free RPE has become a very influential model of dopaminergic (DA) activation in the ventral tegmental area (VTA) and substantia nigra pars compacta (SNc). At the beginning of classical Pavlovian conditioning, DA cells react phasically to unconditioned stimuli (US, rewards). After enough conditioning trials, DA cells only react to conditioned stimuli (CS), i.e. stimuli which predict the delivery of a reward. Moreover, if the reward is omitted, DA cells exhibit a pause in firing. This pattern of activation corresponds to the RPE: DA cells respond to unexpected reward events, either positively when more reward than expected is received, or negatively when less reward is delivered. The simplicity of this model has made RPE a successful model of DA activity (but see <span class="citation" data-cites="Vitay2014">Vitay and Hamker (<a href="#ref-Vitay2014" role="doc-biblioref">2014</a>)</span> for a more detailed model).</p>
<p>A similar but not identical functional dichotomy as MF/MB opposes deliberative <strong>goal-directed</strong> behavior and inflexible stimulus-response associations called <strong>habits</strong> <span class="citation" data-cites="Dickinson2002">(<a href="#ref-Dickinson2002" role="doc-biblioref">Dickinson and Balleine, 2002</a>)</span>. Goal-directed behavior is sensitive to reward devaluation: if an outcome was previously rewarding but ceases to be (for example, a poisonous product is injected into some food reward, even outside the conditioning phase), goal-directed behavior would quickly learn to avoid that outcome, while habitual behavior will continue to seek for it. Over-training can transform goal-directed behavior into habits <span class="citation" data-cites="Corbit2011">(<a href="#ref-Corbit2011" role="doc-biblioref">Corbit and Balleine, 2011</a>)</span>. Habits are usually considered as a model-free learning behavior, while goal-directed behavior implies the use of a world model. The <em>dual system theory</em> discusses the arbitration mechanisms necessary to coordinate these two learning frameworks <span class="citation" data-cites="Lee2014">(<a href="#ref-Lee2014" role="doc-biblioref">Lee et al., 2014</a>)</span>.</p>
<p>Both forms of behavior are thought to happen concurrently in the brain, with model-based / goal-directed behavior classically assigned to the prefrontal cortex and the hippocampus and model-free / habitual behavior mapped to the ventral basal ganglia and the dopaminergic system. However, recent results and theories suggest that these two functional systems are largely overlapping and that even dopamine firing might reflect model-based processes <span class="citation" data-cites="Doll2012 Miller2018">(<a href="#ref-Doll2012" role="doc-biblioref">Doll et al., 2012</a>; <a href="#ref-Miller2018" role="doc-biblioref">Miller et al., 2018</a>)</span>. It is yet to be understood how these two extreme mechanisms of the RL spectrum might coexist in the brain and be coordinated: successor representations might provide us with additional useful insights into the functioning of the brain.</p>
<h1 data-number="2" id="successor-representations-in-reinforcement-learning"><span class="header-section-number">2</span> Successor representations in reinforcement learning</h1>
<h2 data-number="2.1" id="main-idea"><span class="header-section-number">2.1</span> Main idea</h2>
<p>The original formulation of <strong>successor representations</strong> (SR) is actually not recent <span class="citation" data-cites="Dayan1993">(<a href="#ref-Dayan1993" role="doc-biblioref">Dayan, 1993</a>)</span>, but it is subject to a revival since a couple of years with the work of Samuel J. Gershman, e.g. <span class="citation" data-cites="Gershman2012 Gershman2018 Momennejad2017a Stachenfeld2017 Gardner2018">(<a href="#ref-Gardner2018" role="doc-biblioref">Gardner et al., 2018</a>; <a href="#ref-Gershman2012" role="doc-biblioref">Gershman et al., 2012</a>; <a href="#ref-Gershman2018" role="doc-biblioref">Gershman, 2018</a>; <a href="#ref-Momennejad2017a" role="doc-biblioref">Momennejad et al., 2017</a>; <a href="#ref-Stachenfeld2017" role="doc-biblioref">Stachenfeld et al., 2017</a>)</span>.</p>
<p>The SR algorithm learns two quantities:</p>
<ol type="1">
<li>The expected immediate reward received after each state:</li>
</ol>
<p><span class="math display">\[
    r(s) = \mathbb{E}_{\pi} [r_{t+1} | s_t = s]
\]</span></p>
<ol start="2" type="1">
<li>The expected discounted future state occupancy (the <strong>SR</strong> itself):</li>
</ol>
<p><span class="math display">\[
    M(s, s&#39;) = \mathbb{E}_{\pi} [\sum_{k=0}^\infty \gamma^k \, \mathbb{I}(s_{t+k+1} = s&#39;) | s_t = s]
\]</span></p>
<p>I omit here the dependency of <span class="math inline">\(r\)</span> and <span class="math inline">\(M\)</span> on the policy itself in the notation, but it is of course implicitly there.</p>
<p>The SR represents the fact that a state <span class="math inline">\(s&#39;\)</span> can be reached after <span class="math inline">\(s\)</span>, with a value decreasing with the temporal gap between the two states: states occurring in rapid succession will have a high SR, very distant states will have a low SR. If <span class="math inline">\(s&#39;\)</span> happens consistently before <span class="math inline">\(s\)</span>, the SR should be 0 (causality principle). This is in principle similar to model-based RL, but without an explicit representation of the transition structure: it only represents how states are temporally correlated, not which action leads to which state.</p>
<p>The value of a state <span class="math inline">\(s\)</span> is then defined by:</p>
<p><span class="math display">\[
    V^\pi(s) = \sum_{s&#39;} M(s, s&#39;) \, r(s&#39;)
\]</span></p>
<p>The value of a state <span class="math inline">\(s\)</span> depends on which states <span class="math inline">\(s&#39;\)</span> can be visited after it (following the current policy, implicitly), how far in the future they will happen (discount factor in <span class="math inline">\(M(s, s&#39;)\)</span>) and how much reward can be obtained immediately in those states (<span class="math inline">\(r(s&#39;)\)</span>). Note that it is merely a rewriting of the definition of the value of a state, with rewards explicitly separated from state visitation and time replaced by succession probabilities:</p>
<p><span class="math display">\[V^\pi(s_t) = \mathbb{E}_{\pi} [\sum_{k=0} \gamma^k \, r_{t+k+1}] = \mathbb{E}_{\pi} [\sum_{k=0} r(s_{t+k+1}) \times (\gamma^k \, \mathbb{I}(s_{t+k+1}))]\]</span></p>
<p>The SR also obeys a recursive relationship similar to the Bellman equation, as it is based on a discounted sum:</p>
<p><span class="math display">\[
    M(s_t, s&#39;) = \mathbb{I}(s_t = s&#39;) + \gamma \, M(s_{t+1}, s&#39;)
\]</span></p>
<p>The discounted probability of arriving in <span class="math inline">\(s&#39;\)</span> after being in <span class="math inline">\(s\_t\)</span> is one if we are already in <span class="math inline">\(s&#39;\)</span>, and gamma times the discounted probability of arriving in <span class="math inline">\(s&#39;\)</span> after being in the next state <span class="math inline">\(s\_{t+1}\)</span> otherwise.</p>
<p>This recursive relationship implies that we are going to be able to estimate the SR <span class="math inline">\(M(s, s&#39;)\)</span> using a <strong>sensory prediction error</strong> (SPE) similar to the TD RPE <span class="citation" data-cites="Gershman2012">(<a href="#ref-Gershman2012" role="doc-biblioref">Gershman et al., 2012</a>)</span>:</p>
<p><span class="math display">\[
    \delta^\text{SR}_t = \mathbb{I}(s_t = s&#39;) + \gamma \, M(s_{t+1}, s&#39;) - M(s_t, s&#39;)
\]</span></p>
<p><span class="math display">\[
    \Delta M(s_t, s&#39;) = \alpha \, \delta^\text{SR}_t
\]</span></p>
<p>The SPE states that the expected occupancy for states that are visited more frequently than expected (positive sensory prediction error) should be increased, while the expected occupancy for states that are visited less frequently than expected (negative sensory prediction error) should be decreased. In short: is arriving in this new state surprising? It should be noted that the SPE is defined over all possible successor states <span class="math inline">\(s&#39;\)</span>, so the SPE is actually a vector.</p>
<p>We can already observe that SR is a trade-off between MF and MB methods. A change in the reward distribution can be quickly tracked by SR algorithms, as the immediate reward <span class="math inline">\(r(s)\)</span> can be updated with:</p>
<p><span class="math display">\[
    \Delta r(s) = \alpha \, (r_{t+1} - r(s))
\]</span></p>
<p>However, the SR <span class="math inline">\(M(s, s&#39;)\)</span> uses other estimates for its update (bootstrapping), so changes in the transition structure may take more time to propagate to all state-state discounted occupancies <span class="citation" data-cites="Gershman2018">(<a href="#ref-Gershman2018" role="doc-biblioref">Gershman, 2018</a>)</span>.</p>
<h2 data-number="2.2" id="linear-function-approximation"><span class="header-section-number">2.2</span> Linear function approximation</h2>
<p>Before looking at the biological plausibility of this algorithm, we need to deal with the <strong>curse of dimensionality</strong>. The SR <span class="math inline">\(M(s, s&#39;)\)</span> is a matrix associating each state of the system to all other states (size <span class="math inline">\(|\mathcal{S}| \times |\mathcal{S}|\)</span>). This is of course impracticable for most problems and we need to rely on function approximation. The simplest solution is to represent each state <span class="math inline">\(s\)</span> by a set of <span class="math inline">\(d\)</span> features <span class="math inline">\([f\_i(s)]\_{i=1}^d\)</span>. Each feature can for example be the presence of an object in the scene, some encoding of the position of the agent in the world, etc. The SR for a state <span class="math inline">\(s\)</span> only needs to predict the expected discounted probability that a feature <span class="math inline">\(f_j\)</span> will be observed in the future, not the complete state representation. This should ensure generalization across states, as only the presence of relevant features is needed. The SR can be linearly approximated by:</p>
<p><span class="math display">\[
    M_j(s) = \sum_{i=1}^d w_{i, j} \, f_i(s)
\]</span></p>
<p>The expected discounted probability of observing the feature <span class="math inline">\(f\_j\)</span> in the future is defined as a weighted sum of the features of the state <span class="math inline">\(s\)</span>. The value of a state is now defined as:</p>
<p><span class="math display">\[
    V^\pi(s) = \sum_{j=1}^d M_j(s) \, r(f_j) = \sum_{j=1}^d r(f_j) \, \sum_{i=1}^d w_{i, j} \, f_i(s)
\]</span></p>
<p>where <span class="math inline">\(r(f\_j)\)</span> is the expected immediate reward when observing the feature <span class="math inline">\(f\_j\)</span>, what can be easily tracked as before. Computing the value of a state based on the SR now involves a double sum over a <span class="math inline">\(d \times d\)</span> matrix, <span class="math inline">\(d\)</span> being the number of features, what should generally be much more tractable than over the total number of states squared.</p>
<p>As we use linear approximation, the learning rule for the weights <span class="math inline">\(w\_{i, j}\)</span> becomes linearly dependent on the SPE:</p>
<p><span class="math display">\[
    \delta^\text{SR}_t(f_j) = f_j(s_t) + \gamma \, M_j(s_{t+1}) - M_j(s)
\]</span></p>
<p><span class="math display">\[
    \Delta w_{i, j} = \alpha \, \delta^\text{SR}_t(f_j) \, f_i(s_t)
\]</span></p>
<p>The SPE tells us how surprising is each feature <span class="math inline">\(f\_j\)</span> when being in the state <span class="math inline">\(s_t\)</span>. This explains the term <em>sensory prediction error</em>: we are now not learning based on how surprising rewards are anymore, but on how surprising the sensory features of the outcome are. Did I expect that door to open at some point? Should this event happen soon? What kind of outcome is likely to happen? As the SPE is now a vector for all sensory features, we see why successor representation have a great potential: instead of a single scalar RPE dealing only with reward magnitudes, we now can learn from very diverse representations describing the various relevant dimensions of the task. It can then deal with different rewards: food and monetary rewards are treated the same by RPEs, while we can distinguish them with SPEs.</p>
<p>The main potential problem is of course to extract the relevant features for the task, either by hand-engineering them or through learning (one could work in the latent space of a variational autoencoder, for example). Feature-based state representations still have to be Markovian for SR to work. It is also possible to use non-linear function approximators such as deep networks <span class="citation" data-cites="Kulkarni2016 Barreto2016 Zhang2016 Machado2018 Ma2018">(<a href="#ref-Barreto2016" role="doc-biblioref">Barreto et al., 2016</a>; <a href="#ref-Kulkarni2016" role="doc-biblioref">Kulkarni et al., 2016</a>; <a href="#ref-Ma2018" role="doc-biblioref">Ma et al., 2018</a>; <a href="#ref-Machado2018" role="doc-biblioref">Machado et al., 2018</a>; <a href="#ref-Zhang2016" role="doc-biblioref">Zhang et al., 2016</a>)</span>, but this is out of the scope of this post.</p>
<h2 data-number="2.3" id="successor-representations-of-actions"><span class="header-section-number">2.3</span> Successor representations of actions</h2>
<p>The previous sections focused on the successor representation of states to obtain the value function <span class="math inline">\(V^\pi(s)\)</span>. The same idea can be applied to state-action pairs and their <span class="math inline">\(Q^\pi(s, a)\)</span> values. The Q-value of a state action pair can be defined as:</p>
<p><span class="math display">\[
    Q^\pi(s, a) = \sum_{s&#39;, a&#39;} M(s, a, s&#39;, a&#39;) \, r(s&#39;, a&#39;)
\]</span></p>
<p>where <span class="math inline">\(r(s&#39;, a&#39;)\)</span> is the expected immediate reward obtained after <span class="math inline">\((s&#39;, a&#39;)\)</span> and <span class="math inline">\(M(s, a, s&#39;, a&#39;)\)</span> is the SR between the pairs <span class="math inline">\((s, a)\)</span> and <span class="math inline">\((s&#39;, a&#39;)\)</span> as in (Momennejad et al., 2017). Ducarouge and Sigaud (2017) use a SR representation between a state-action pair <span class="math inline">\((s, a)\)</span> and a successor state <span class="math inline">\(s&#39;\)</span>:</p>
<p><span class="math display">\[
    Q^\pi(s, a) = \sum_{s&#39;} M(s, a, s&#39;) \, r(s&#39;)
\]</span></p>
<p>In both cases, the SR can be learned using a sensory prediction error, such as:</p>
<p><span class="math display">\[
    \delta^\text{SR}_{s_t, a_t} = \mathbb{I}(s_t = s&#39;) + \gamma \, M(s_{t+1}, a_{t+1}, s&#39;) - M(s_t, a_t, s&#39;)
\]</span></p>
<p>Note that eligibility traces can be used in SR learning as easily as in TD methods.</p>
<h1 data-number="3" id="successor-representations-in-neuroscience"><span class="header-section-number">3</span> Successor representations in neuroscience</h1>
<h2 data-number="3.1" id="human-goal-directed-behavior"><span class="header-section-number">3.1</span> Human goal-directed behavior</h2>
<p>So great, we now have a third form of reinforcement learning. Could it be the missing theory to explain human reinforcement learning and the dichotomy goal-directed behavior / habits?</p>
<p>Momennejad et al. (2017) designed a two-steps sequential learning task with reward and transition revaluations. In the first learning phase, the subjects are presented with sequences of images (the states) and obtain different rewards (Fig. 1). The sequence <span class="math inline">\(1 \rightarrow 3 \rightarrow 5\)</span> is rewarded with 10 dollars while the sequence <span class="math inline">\(2 \rightarrow 4 \rightarrow 6\)</span> is rewarded with 1 dollar only. Successful learning is tested by asking the participant whether he/she prefers the states 1 or 2 (the answer is obviously 1).</p>
<figure>
<img src="img/sr_momennejad.svg" style="width:60.0%" alt="Figure 1. Two-steps sequential learning task of Momennejad et al. (2017)." /><figcaption aria-hidden="true"><strong>Figure 1.</strong> Two-steps sequential learning task of Momennejad et al. (2017).</figcaption>
</figure>
<p>In the reward revaluation task, the transitions <span class="math inline">\(3 \rightarrow 5\)</span> and <span class="math inline">\(4 \rightarrow 6\)</span> are experienced again in the re-learning phase, but this time with reversed rewards (1 and 10 dollars respectively). In the transition revaluation task, the transitions <span class="math inline">\(3 \rightarrow 6\)</span> and <span class="math inline">\(4 \rightarrow 5\)</span> are now experienced, but the states <span class="math inline">\(5\)</span> and <span class="math inline">\(6\)</span> still receive the same amount of reward. The preference for <span class="math inline">\(1\)</span> or <span class="math inline">\(2\)</span> is again tested at the end of the re-learning phase (<span class="math inline">\(2\)</span> should now be preferred in both tasks) and a revaluation score is computed (how much the subject changes his preference between the two phases).</p>
<p>What would the different ML methods predict?</p>
<ul>
<li><p>Model-free methods would not change their preference in both conditions. The value of the <span class="math inline">\(3 \rightarrow 5\)</span> and <span class="math inline">\(4 \rightarrow 6\)</span> transitions (reward revaluation) or <span class="math inline">\(3 \rightarrow 6\)</span> and <span class="math inline">\(4 \rightarrow 5\)</span> (transition revaluation) would change during the re-learning phase, but the transitions <span class="math inline">\(1 \rightarrow 3\)</span> and <span class="math inline">\(2 \rightarrow 4\)</span> are never experienced again, so the value of the states <span class="math inline">\(1\)</span> and <span class="math inline">\(2\)</span> can only stay the same, even with eligibility traces.</p></li>
<li><p>Model-based methods would change their preference in both conditions. The reward and transition probabilities would both be re-learned completely to reflect the change, so the new value of <span class="math inline">\(1\)</span> and <span class="math inline">\(2\)</span> can be computed correctly using dynamic programming.</p></li>
<li><p>Successor representation methods would adapt to the reward revaluation (<span class="math inline">\(r(s)\)</span> will quickly fit the new reward distribution for the states <span class="math inline">\(5\)</span> and <span class="math inline">\(6\)</span>), but not to the transition revaluation: <span class="math inline">\(6\)</span> is never a successor state of <span class="math inline">\(1\)</span> in the re-learning phase, so the SR matrix will not be updated for the states <span class="math inline">\(1\)</span> and <span class="math inline">\(2\)</span>.</p></li>
</ul>
<p>We have three different mechanisms with testable predictions on these two tasks: the human experiments should tell us which method is the best model of human RL. Well… Not really.</p>
<figure>
<img src="img/sr_results.png" alt="Figure 2. Revaluation score in the reward (red) and transition (blue) revaluation conditions for the model-free (MF), model-based (MB), successor representation (SR) and human data as reported in Momennejad et al. (2017)." /><figcaption aria-hidden="true"><strong>Figure 2.</strong> Revaluation score in the reward (red) and transition (blue) revaluation conditions for the model-free (MF), model-based (MB), successor representation (SR) and human data as reported in Momennejad et al. (2017).</figcaption>
</figure>
<p>Human participants show a revaluation behavior in the two conditions (reward and transition) somehow in between the model-based and successor representation algorithms. The difference between the reward and transition conditions is statistically significant, so unlike MB, but not as dramatic as for SR. The authors propose a hybrid SR-MB model, linearly combining the outputs of the MB and SR algorithms, and fit it to the human data to obtain a satisfying match. A second task requiring the model to actually take actions confirms this observation.</p>
<p>It is hard to conclude anything definitive from this model and the somehow artificial fit to the data. Reward revaluation was the typical test to distinguish between MB and MF processes, or between goal-directed behavior and habits. This paper suggests that transition revaluation (and policy revaluation, investigated in the second experiment) might allow distinguishing between MB and SR mechanisms, supporting the existence of SR mechanisms in the brain. How MB and SR might interact in the brain and whether there is an arbitration mechanism between the two is still an open issue. (Russek et al., 2017) has a very interesting discussion on the link between MF and MB processes in the brain, based on different versions of the SR.</p>
<h2 data-number="3.2" id="neural-substrates-of-successor-representations"><span class="header-section-number">3.2</span> Neural substrates of successor representations</h2>
<p>In addition to describing human behavior at the functional level, the SR might also allow to better understand the computations made by the areas involved in goal-directed behavior, in particular the prefrontal cortex, the basal ganglia, the dopaminergic system, and the hippocampus. The key idea of Gershman and colleagues is that the SR <span class="math inline">\(M(s, s&#39;)\)</span> might be encoded in the place cells of the hippocampus (Stachenfeld et al., 2017), which are known to be critical for reward-based navigation. The sensory prediction error (SPE <span class="math inline">\(\delta^\text{SR}\_t\)</span>) might be encoded in the activation of the dopaminergic cells in VTA (or in a fronto-striatal network), driving learning of the SR in the hippocampus (Gardner et al., 2018), while the value of a state <span class="math inline">\(V^\pi(s) = \sum_{s&#39;} M(s, s&#39;) \, r(s&#39;)\)</span> could be computed either in the prefrontal cortex (ventromedial or orbitofrontal) or in the ventral striatum (nucleus accumbens in rats), ultimately allowing action selection in the dorsal BG.</p>
<h3 data-number="3.2.1" id="dopamine-as-a-spe"><span class="header-section-number">3.2.1</span> Dopamine as a SPE</h3>
<p>The most striking prediction of the SR hypothesis is that the SPE is a vector of prediction errors, with one element per state (in the original formulation) or per reward feature (using linear function approximation, section 2.2). This contrasts with the classical RPE formulation, where dopaminergic activation is a single scalar signal driving reinforcement learning in the BG and prefrontal cortex. Although this would certainly be an advantage in terms of functionality and flexible learning, it remains to be shown whether VTA actually encodes such a feature-specific signal.</p>
<p>Neurons in VTA have a rather uniform response to rewards or reward-predicting cues, encoding mostly the value of the outcome regardless its sensory features, except for those projecting to the tail of the striatum which mostly respond to threats and punishments (Watabe-Uchida and Uchida, 2019). The current state of knowledge seems to rule out VTA as a direct source of SPE signals.</p>
<p>Interestingly, Oemisch et al. (2019) showed that feature-specific prediction errors signals (analogous to the SPE with linear approximation) are detected in the fronto-striatal network including the anterior cingulate area (ACC), dorsolateral prefrontal cortex (dlPFC), dorsal striatum and ventral striatum (VS) / nucleus accumbens (NAcc). These SPE-like signals appear shortly after non-specific RPE signals, first in ACC and then in the rest of the network. This suggests that SPE would actually be the result of a more complex calculation than proposed in the SR hypothesis, involving a network of interconnected areas. A detailed neuro-computational model of this network still has to be proposed.</p>
<h3 data-number="3.2.2" id="hippocampus-as-a-predictive-map"><span class="header-section-number">3.2.2</span> Hippocampus as a predictive map</h3>
<p>Another interesting prediction of the SR hypothesis is that the hippocampus might be the site where the SR matrix is represented (Stachenfeld et al., 2017). In navigation tasks, the so-called <strong>place cells</strong> in the hippocampus exhibit roughly circular receptive fields centered on different locations in the environment (O’Keefe and Nadel, 1978). Altogether, place cells are thought to provide a sparse code of the animal’s location. Strikingly, place fields change with the environment: moving the animal from a circular to a rectangular environment, or introducing barriers, modifies the distribution of place fields. Additionally, <strong>grid cells</strong> in the entorhinal cortex (reciprocally connected to the hippocampus) show a hexagonal grid pattern of receptive fields, i.e. a single grid cell responds for several positions of the animal inside the environment (Hafting et al., 2005). Grid cells’ receptive fields also depend on the environment and have been shown to depend on place cells, not the other way around. The mechanism behind the flexibility of place and grid fields is still to be understood.</p>
<p>Stachenfeld et al. (2017) propose that place cells actually encode the SR <span class="math inline">\(M(s, s&#39;)\)</span> between the current location <span class="math inline">\(s\)</span> and their preferred location <span class="math inline">\(s&#39;\)</span>, rather than simply an Euclidian distance between <span class="math inline">\(s\)</span> and <span class="math inline">\(s&#39;\)</span> as classically used in hippocampal models. Because of the discount rate in the SR and its dependency on the animal’s policy, place fields are then roughly circular (exponentially decreasing) in an open environment, where the animal can theoretically reach any neighboring location from its current position. When constraints are added to the environment, such as walls and barriers, certain transitions are not possible anymore, which will modify the shape of the place fields. This fits with experimental observations, contrary to most models of place field formation using Gaussian receptive fields around fixed locations. Additionally, the SR hypothesis is in agreement with the observation that rewarded locations are represented by a higher number of place cells, as the animal spends more time around them.</p>
<figure>
<img src="img/sr_track.png" alt="Figure 3. Place field on a linear track with an obstacle at x=1, using a Euclidian model (left) and the SR hypothesis (right). Adapted from Fig. 2 of (Stachenfeld et al., 2017)." /><figcaption aria-hidden="true"><strong>Figure 3.</strong> Place field on a linear track with an obstacle at x=1, using a Euclidian model (left) and the SR hypothesis (right). Adapted from Fig. 2 of (Stachenfeld et al., 2017).</figcaption>
</figure>
<p>Fig. 3 illustrates this prediction: if a rat is placed on a linear track with an obstacle, the place cell whose RF is centered on the obstacle would react identically on both sides of the obstacle using a Euclidian model, while it would only respond on the side it has explored using the SR. When the rat is put on the other side, the SRs would initially be 0 for that cell (but would grow with more exploration).</p>
<p>Stachenfeld et al. (2017) also propose a mechanism for grid cell formation in the entorhinal cortex. Grid cells are understood as a low-dimensional eigendecomposition of the SR place cells (dimensionality reduction, as in principal component analysis). This allows to explain why grid cells change in different environments (circular, rectangular or triangular), as experimentally observed. They also propose a mechanism for sub-goal formation using grid cells, but using the normalized min-cut algorithm, so quite far from being biologically realistic.</p>
<h1 data-number="4" id="discussion"><span class="header-section-number">4</span> Discussion</h1>
<p>Successor representations are an interesting trade-off between model-free and model-based RL algorithms, explicitly separating state transitions from reward estimation. It allows reacting quickly to distal reward changes without the computational burden of completely model-based planning. Deep RL variants of SR (Kulkarni et al., 2016) obtain satisfying results on classical RL tasks such as Atari games and simulated robots, but are still outperformed by modern model-free algorithms. Similar to human behavior, hybrid architectures using both SR and MF methods might be able to combine the optimality of MF methods with the flexibility of the SR.</p>
<p>At the neuroscientific level, the SR hypothesis raises a lot of interesting questions, especially regarding the interplay between the prefrontal cortex, the hippocampus, and the basal ganglia during goal-directed behavior. Here are just a few aspects that need to be investigated both experimentally and theoretically:</p>
<ul>
<li>What is the relationship between the RPE and the SPE? Does VTA compute the SPE (still to be proven) and send it directly to the hippocampus through dopaminergic projections? Or does the RPE VTA somehow “train” ACC and PFC to compute the SPE, what is then sent to the hippocampus to update the SR representation? How?</li>
<li>How does the hippocampus learn from SPE signals? The SR hypothesis still has to be linked with evidence on plasticity in the hippocampus.</li>
<li>If dopamine does not carry the SPE, what is the role of the dopaminergic innervation of the hippocampus? The SR representation is in principle independent from rewards (except that animals may spend more time around reward location).</li>
<li>How is the value of a state / action computed based on the SR representation in the hippocampus? Do sharp wave ripples (SWR, also called forward/inverse replays) actually sample the SR matrix (a list of achievable states from the current one), what is then integrated elsewhere (ventral striatum?) to guide behavior?</li>
</ul>
<h1 class="unnumbered" id="references">References</h1>
<div id="refs" class="references csl-bib-body hanging-indent" role="doc-bibliography">
<div id="ref-Barreto2016" class="csl-entry" role="doc-biblioentry">
Barreto, A., Dabney, W., Munos, R., Hunt, J. J., Schaul, T., van Hasselt, H., et al. (2016). Successor <span>Features</span> for <span>Transfer</span> in <span>Reinforcement Learning</span>. <em>arXiv:1606.05312 [cs]</em>. Available at: <a href="http://arxiv.org/abs/1606.05312">http://arxiv.org/abs/1606.05312</a> [Accessed March 11, 2019].
</div>
<div id="ref-Corbit2011" class="csl-entry" role="doc-biblioentry">
Corbit, L. H., and Balleine, B. W. (2011). The general and outcome-specific forms of <span>Pavlovian</span>-instrumental transfer are differentially mediated by the nucleus accumbens core and shell. <em>The Journal of neuroscience : the official journal of the Society for Neuroscience</em> 31, 11786–94. doi:<a href="https://doi.org/10.1523/JNEUROSCI.2711-11.2011">10.1523/JNEUROSCI.2711-11.2011</a>.
</div>
<div id="ref-Dayan1993" class="csl-entry" role="doc-biblioentry">
Dayan, P. (1993). Improving <span>Generalization</span> for <span>Temporal Difference Learning</span>: The <span>Successor Representation</span>. <em>Neural Computation</em> 5, 613–624. doi:<a href="https://doi.org/10.1162/neco.1993.5.4.613">10.1162/neco.1993.5.4.613</a>.
</div>
<div id="ref-Dickinson2002" class="csl-entry" role="doc-biblioentry">
Dickinson, A., and Balleine, B. (2002). <span>“The role of learning in the operation of motivational systems,”</span> in <em>Steven’s handbook of experimental psychology: Learning, motivation, and emotion, <span>Vol</span>. 3, 3rd ed</em> (<span>Hoboken, NJ, US</span>: <span>John Wiley &amp; Sons Inc</span>), 497–533.
</div>
<div id="ref-Doll2012" class="csl-entry" role="doc-biblioentry">
Doll, B. B., Simon, D. A., and Daw, N. D. (2012). The ubiquity of model-based reinforcement learning. <em>Current Opinion in Neurobiology</em> 22, 1075–1081. doi:<a href="https://doi.org/10.1016/j.conb.2012.08.003">10.1016/j.conb.2012.08.003</a>.
</div>
<div id="ref-Gardner2018" class="csl-entry" role="doc-biblioentry">
Gardner, M. P. H., Schoenbaum, G., and Gershman, S. J. (2018). Rethinking dopamine as generalized prediction error. <em>Proceedings of the Royal Society B: Biological Sciences</em> 285, 20181645. doi:<a href="https://doi.org/10.1098/rspb.2018.1645">10.1098/rspb.2018.1645</a>.
</div>
<div id="ref-Gershman2018" class="csl-entry" role="doc-biblioentry">
Gershman, S. J. (2018). The <span>Successor Representation</span>: Its <span>Computational Logic</span> and <span>Neural Substrates</span>. <em>The Journal of neuroscience : the official journal of the Society for Neuroscience</em> 38, 7193–7200. doi:<a href="https://doi.org/10.1523/JNEUROSCI.0151-18.2018">10.1523/JNEUROSCI.0151-18.2018</a>.
</div>
<div id="ref-Gershman2012" class="csl-entry" role="doc-biblioentry">
Gershman, S. J., Moore, C. D., Todd, M. T., Norman, K. A., and Sederberg, P. B. (2012). The <span>Successor Representation</span> and <span>Temporal Context</span>. <em>Neural Computation</em> 24, 1553–1568. doi:<a href="https://doi.org/10.1162/NECO_a_00282">10.1162/NECO_a_00282</a>.
</div>
<div id="ref-Kulkarni2016" class="csl-entry" role="doc-biblioentry">
Kulkarni, T. D., Saeedi, A., Gautam, S., and Gershman, S. J. (2016). Deep <span>Successor Reinforcement Learning</span>. <em>arXiv:1606.02396 [cs, stat]</em>. Available at: <a href="http://arxiv.org/abs/1606.02396">http://arxiv.org/abs/1606.02396</a> [Accessed February 20, 2019].
</div>
<div id="ref-Lee2014" class="csl-entry" role="doc-biblioentry">
Lee, S. W., Shimojo, S., and O’Doherty, J. P. (2014). Neural <span>Computations Underlying Arbitration</span> between <span>Model</span>-<span>Based</span> and <span>Model</span>-free <span>Learning</span>. <em>Neuron</em> 81, 687–699. doi:<a href="https://doi.org/10.1016/j.neuron.2013.11.028">10.1016/j.neuron.2013.11.028</a>.
</div>
<div id="ref-Ma2018" class="csl-entry" role="doc-biblioentry">
Ma, C., Wen, J., and Bengio, Y. (2018). Universal <span>Successor Representations</span> for <span>Transfer Reinforcement Learning</span>. <em>arXiv:1804.03758 [cs, stat]</em>. Available at: <a href="http://arxiv.org/abs/1804.03758">http://arxiv.org/abs/1804.03758</a> [Accessed February 23, 2019].
</div>
<div id="ref-Machado2018" class="csl-entry" role="doc-biblioentry">
Machado, M. C., Bellemare, M. G., and Bowling, M. (2018). Count-<span>Based Exploration</span> with the <span>Successor Representation</span>. <em>arXiv:1807.11622 [cs, stat]</em>. Available at: <a href="http://arxiv.org/abs/1807.11622">http://arxiv.org/abs/1807.11622</a> [Accessed February 23, 2019].
</div>
<div id="ref-Miller2018" class="csl-entry" role="doc-biblioentry">
Miller, K., Ludvig, E. A., Pezzulo, G., and Shenhav, A. (2018). <span>“Re-aligning models of habitual and goal-directed decision-making,”</span> in <em>Goal-<span>Directed Decision Making</span> : Computations and <span>Neural Circuits</span></em>, eds. A. Bornstein, R. W. Morris, and A. Shenhav (<span>Academic Press</span>). Available at: <a href="https://www.elsevier.com/books/goal-directed-decision-making/morris/978-0-12-812098-9">https://www.elsevier.com/books/goal-directed-decision-making/morris/978-0-12-812098-9</a> [Accessed December 11, 2018].
</div>
<div id="ref-Momennejad2017a" class="csl-entry" role="doc-biblioentry">
Momennejad, I., Russek, E. M., Cheong, J. H., Botvinick, M. M., Daw, N. D., and Gershman, S. J. (2017). The successor representation in human reinforcement learning. <em>Nature Human Behaviour</em> 1, 680–692. doi:<a href="https://doi.org/10.1038/s41562-017-0180-8">10.1038/s41562-017-0180-8</a>.
</div>
<div id="ref-Stachenfeld2017" class="csl-entry" role="doc-biblioentry">
Stachenfeld, K. L., Botvinick, M. M., and Gershman, S. J. (2017). The hippocampus as a predictive map. <em>Nature Neuroscience</em> 20, 1643–1653. doi:<a href="https://doi.org/10.1038/nn.4650">10.1038/nn.4650</a>.
</div>
<div id="ref-Sutton2017" class="csl-entry" role="doc-biblioentry">
Sutton, R. S., and Barto, A. G. (2017). <em>Reinforcement <span>Learning</span>: An <span>Introduction</span></em>. Second. <span>Cambridge, MA</span>: <span>MIT Press</span> Available at: <a href="http://incompleteideas.net/book/the-book-2nd.html">http://incompleteideas.net/book/the-book-2nd.html</a>.
</div>
<div id="ref-Vitay2014" class="csl-entry" role="doc-biblioentry">
Vitay, J., and Hamker, F. H. (2014). Timing and expectation of reward: A neuro-computational model of the afferents to the ventral tegmental area. <em>Frontiers in Neurorobotics</em> 8. doi:<a href="https://doi.org/10.3389/fnbot.2014.00004">10.3389/fnbot.2014.00004</a>.
</div>
<div id="ref-Zhang2016" class="csl-entry" role="doc-biblioentry">
Zhang, J., Springenberg, J. T., Boedecker, J., and Burgard, W. (2016). Deep <span>Reinforcement Learning</span> with <span>Successor Features</span> for <span>Navigation</span> across <span>Similar Environments</span>. <em>arXiv:1612.05533 [cs]</em>. Available at: <a href="http://arxiv.org/abs/1612.05533">http://arxiv.org/abs/1612.05533</a> [Accessed March 11, 2019].
</div>
</div>
</article>
</body>
</html>